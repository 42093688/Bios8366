{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width: 90%;\n",
       "/*        margin-left:auto;*/\n",
       "/*        margin-right:auto;*/\n",
       "    }\n",
       "    ul {\n",
       "        line-height: 145%;\n",
       "        font-size: 90%;\n",
       "    }\n",
       "    li {\n",
       "        margin-bottom: 1em;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top: 12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width: 90%;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "/*    .prompt{\n",
       "        display: None;\n",
       "    }*/\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 16pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from scipy import optimize\n",
    "from ipywidgets import *\n",
    "\n",
    "theano.config.compute_test_value = 'ignore'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch and Pitts Neuron\n",
    "\n",
    "In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:\n",
    "\n",
    "1. A set of **weights** $w_i$ corresponding to synapses (inputs)\n",
    "2. An **adder** for summing input signals; analogous to cell membrane that collects charge\n",
    "3. An **activation function** for determining when the neuron fires, based on accumulated input\n",
    "\n",
    "The neuron model is shown schematically below. On the left are input nodes $\\{x_i\\}$, usually expressed as a vector. The strength with which the inputs are able to deliver the signal along the synapse is determined by their corresponding weights $\\{w_i\\}$. The adder then sums the inputs from all the synapses:\n",
    "\n",
    "$$h = \\sum_i w_i x_i$$\n",
    "\n",
    "The parameter $\\theta$ determines whether or not the neuron fires given a weighted input of $h$. If it fires, it returns a value $y=1$, otherwise $y=0$. For example, a simple **activation function** is using $\\theta$ as a simple fixed threshold:\n",
    "\n",
    "$$y = g(h) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt \\theta \\\\\n",
    "0, \\text{if } h \\le \\theta\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "this activation function may take any of several forms, such as a logistic function.\n",
    "\n",
    "![neuron](http://d.pr/i/9AMK+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a **network** can they perform useful work.\n",
    "\n",
    "Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.\n",
    "\n",
    "Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.\n",
    "\n",
    "The number of inputs and outputs are determined by the data. Weights are stored as a `N x K` matrix, with N observations and K neurons, with $w_{ij}$ specifying the weight on the *i*th observation on the *j*th neuron.\n",
    "\n",
    "![perceptron](http://d.pr/i/4IWA+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the perceptron for statistical learning, we compare the outputs $y_j$ from each neuron to the obervation target $t_j$, and adjust the input weights when they do not correspond (*e.g.* if a neuron fires when it should not have).\n",
    "\n",
    "$$t_j - y_j$$\n",
    "\n",
    "We use this difference to update the weight $w_{ij}$, based on the input and a desired **learning rate**. This results in an update rule:\n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "After an incremental improvement, the perceptron is shown the training data again, resulting in another update. This is repeated until the performance no longer improves. Having a learning rate less than one results in a more stable learning rate, though this stability is traded off against having to expose the network to the data multiple times. Typical learning rates are in the 0.1-0.4 range.\n",
    "\n",
    "An additional input node is typically added to the perceptron model, which is a constant value (usually -1, 0, or 1) that acts analogously to an intercept in a regression model. This establishes a baseline input for the case when all inputs are zero.\n",
    "\n",
    "![bias](http://d.pr/i/105b5+)\n",
    "\n",
    "## Learning with Perceptrons\n",
    "\n",
    "1. Initialize weights $w_{ij}$ to small, random numbers.\n",
    "2. For each t in T iterations\n",
    "    * compute activation for each neuron *j* connected to each input vector *i*\n",
    "    $$y_j = g\\left( h=\\sum_i w_{ij} x_i \\right) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt 0 \\\\\n",
    "0, \\text{if } h \\le 0\n",
    "\\end{array} \\right.$$\n",
    "    * update weights\n",
    "    $$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "\n",
    "This algorithm is $\\mathcal{O}(Tmn)$\n",
    "\n",
    "### Example: Logical functions\n",
    "\n",
    "Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables `x1` and `x2`, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  0\n",
       "2   1   0  0\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\n",
    "AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize weights to small, random values (can be positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a simple activation function for calculating $g(h)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a training function that iterates the learning algorihtm, returning the adapted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, eta, n_iterations):\n",
    "\n",
    "    # Add the inputs that match the bias node\n",
    "    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n",
    "\n",
    "    for n in range(n_iterations):\n",
    "\n",
    "        activations = g(inputs, weights);\n",
    "        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it first on the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = AND[['x1','x2']]\n",
    "target = AND['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it has learned the function perfectly. Now for OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\n",
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = OR[['x1','x2']]\n",
    "target = OR['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also 100% correct.\n",
    "\n",
    "### Exercise: XOR\n",
    "\n",
    "Now try running the model on the XOR function, where a one is returned for either `x1` or `x2` being true, but *not* both. What happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the problem graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1096a5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109786ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(-.4,1), .5 - 1*np.linspace(-.4,1), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wDuTCt2+V3K4QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109d00358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n",
    "\n",
    "XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron tries to find a separating hyperplane for the two response classes. Namely, a set of weights that satisfies:\n",
    "\n",
    "$$\\mathbf{x_1}\\mathbf{w}^T=0$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{x_2}\\mathbf{w}^T=0$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{x}_1\\mathbf{w}^T &= \\mathbf{x}_2\\mathbf{w}^T \\\\\n",
    "\\Rightarrow (\\mathbf{x}_1 - \\mathbf{x}_2) \\mathbf{w}^T &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that either the norms of $\\mathbf{x}_1 - \\mathbf{x}_2$ or $\\mathbf{w}$ are zero, or the cosine of the angle between them is equal to zero, due to the identity:\n",
    "\n",
    "$$\\mathbf{a}\\mathbf{b} = \\|a\\| \\|b\\| \\cos \\theta$$\n",
    "\n",
    "Since there is no reason for the norms to be zero in general, we need the two vectors to be at right angles to one another. So, we need a weight vector that is perpendicular to the decision boundary.\n",
    "\n",
    "Clearly, for the XOR function, the output classes are not linearly separable. So, the algorithm does not converge on an answer, but simply cycles through two incorrect solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply add more weights.\n",
    "\n",
    "There are two ways to add complexity:\n",
    "\n",
    "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
    "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
    "\n",
    "The latter approach is more common in applications of neural networks.\n",
    "\n",
    "![multilayer](http://d.pr/i/14BS1+)\n",
    "\n",
    "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
    "\n",
    "Updating a multi-layer perceptron (MLP) is a matter of: \n",
    "\n",
    "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
    "2. moving backward updating weights according to the resulting error from forward propagation. \n",
    "\n",
    "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer (in principle, we can add additional hidden layers, but without sacrificing generality, I will keep it simple).\n",
    "\n",
    "### Error back-propagation\n",
    "\n",
    "We update the weights in a MLP using **back-propagation** of the prediction errors, which is essentially a form of gradient descent, as we have used previously for optimization.\n",
    "\n",
    "First, for the multi-layer perceptron we need to modify the error function, which in the single-layer case was a simple difference between the predicted and observed outputs. Because we will be summing errors, we have to avoid having errors in different directions cancelling each other out, so a sum of squares error is more appropriate:\n",
    "\n",
    "$$E(t,y) = \\frac{1}{2} \\sum_i (t_i - y_i)^2$$\n",
    "\n",
    "It is on this function that we will perform gradient descent, since the goal is to minimize the error. Specificially, we will differentiate with respect to the weights, since it is the weights that we are manipulating in order to get better predictions.\n",
    "\n",
    "Recall that the error is a function of the threshold function\n",
    "\n",
    "$$E(\\mathbf{w}) = \\frac{1}{2} \\sum_i (t_i - y_i)^2 = \\frac{1}{2} \\sum_i \\left(t_i - g\\left[ \\sum_j w_{ij} a_j \\right]\\right)^2$$\n",
    "\n",
    "So, we will also need to differentiate that. However, the threshold function we used in the single-layer perceptron was discontinuous, making it non-differentiable. Thus, we need to modify it as well. An alternative is to employ some type of sigmoid function, such as the logistic, which can be parameterized to resemble a threshold function, but varies smoothly across its range.\n",
    "\n",
    "$$g(h) = \\frac{1}{1 + \\exp(-\\beta h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAegAAAFVCAYAAAAkBHynAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108ea5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n",
    "\n",
    "@interact\n",
    "def logistic_plot(beta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    plt.plot(hvals, logistic(hvals, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the advantage of having a simple derivative:\n",
    "\n",
    "$$\\frac{dg}{dh} = \\beta g(h)(1 - g(h))$$\n",
    "\n",
    "Alternatively, the hyperbolic tangent function is also sigmoid:\n",
    "\n",
    "$$g(h) = \\tanh(h) = \\frac{\\exp(h) - \\exp(-h)}{\\exp(h) + \\exp(-h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAFVCAYAAAApGgzgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCMENAICNENwAANjI/wd2Owdy6tegiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108edd080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n",
    "\n",
    "@interact\n",
    "def tanh_plot(theta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    h = hvals*theta\n",
    "    plt.plot(hvals, hyperbolic_tangent(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "\n",
    "$$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "The second term in the chain rule simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "For the first term in the chain rule above, we decompose it as well:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "We can now outline the MLP learning algorithm:\n",
    "\n",
    "1. Initialize all $w_{jk}$ to small random values\n",
    "2. Iterate until learning completes:\n",
    "    + For each input vector, run the network forward:\n",
    "        * compute activation of each neuron $j$ in hidden layer:\n",
    "        $$h_j = \\sum_i x_i v_{ij}$$\n",
    "        $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "        * when the output layer is reached, calculate outputs similarly:\n",
    "        $$h_k = \\sum_k a_j w_{jk}$$\n",
    "        $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "    + Then run the network backward:\n",
    "        * compute error at output:\n",
    "        $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "        * compute error of the hidden layers:\n",
    "        $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "        * update output layer weights:\n",
    "        $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "        * update hidden layer weights:\n",
    "        $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    + For best performance, shuffle input vectors to avoid training in the same order\n",
    "\n",
    "Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "A weight update with momentum looks like this:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "\n",
    "where $\\alpha$ is the momentum parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration.\n",
    "\n",
    "The multi-layer pereptron is implemented below in the `MLP` class. The implementation uses the scikit-learn interface, so it is uses in the same way as other supervised learning algorithms in that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, \n",
    "                 method='CG', maxiter=500):\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.activation_func = self.sigmoid\n",
    "        self.activation_func_prime = self.sigmoid_prime\n",
    "        self.method = method\n",
    "        self.maxiter = maxiter\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    def sumsqr(self, a):\n",
    "        return np.sum(a ** 2)\n",
    "    \n",
    "    def rand_init(self, l_in, l_out):\n",
    "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
    "    \n",
    "    def pack_thetas(self, t1, t2):\n",
    "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
    "    \n",
    "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
    "        t1_start = 0\n",
    "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
    "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
    "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
    "        return t1, t2\n",
    "    \n",
    "    def _forward(self, X, t1, t2):\n",
    "        m = X.shape[0]\n",
    "        ones = None\n",
    "        if len(X.shape) == 1:\n",
    "            ones = np.array(1).reshape(1,)\n",
    "        else:\n",
    "            ones = np.ones(m).reshape(m,1)\n",
    "        \n",
    "        # Input layer\n",
    "        a1 = np.hstack((ones, X))\n",
    "        \n",
    "        # Hidden Layer\n",
    "        z2 = np.dot(t1, a1.T)\n",
    "        a2 = self.activation_func(z2)\n",
    "        a2 = np.hstack((ones, a2.T))\n",
    "        \n",
    "        # Output layer\n",
    "        z3 = np.dot(t2, a2.T)\n",
    "        a3 = self.activation_func(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        \n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "        \n",
    "        *unused_out, h = self._forward(X, t1, t2)\n",
    "        costPositive = -Y * np.log(h).T\n",
    "        costNegative = (1 - Y) * np.log(1 - h).T\n",
    "        cost = costPositive - costNegative\n",
    "        J = np.sum(cost) / m\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            t1f = t1[:, 1:]\n",
    "            t2f = t2[:, 1:]\n",
    "            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))\n",
    "            J += reg\n",
    "            \n",
    "        return J\n",
    "        \n",
    "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
    "        \n",
    "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        t1f = t1[:, 1:]\n",
    "        t2f = t2[:, 1:]\n",
    "        Y = np.eye(num_labels)[y]\n",
    "        \n",
    "        Delta1, Delta2 = 0, 0\n",
    "        for i, row in enumerate(X):\n",
    "            \n",
    "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
    "            \n",
    "            # Backprop\n",
    "            d3 = a3 - Y[i, :].T\n",
    "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
    "            \n",
    "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
    "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
    "            \n",
    "        Theta1_grad = (1 / m) * Delta1\n",
    "        Theta2_grad = (1 / m) * Delta2\n",
    "        \n",
    "        if reg_lambda != 0:\n",
    "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
    "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
    "        \n",
    "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        num_features = X.shape[0]\n",
    "        input_layer_size = X.shape[1]\n",
    "        num_labels = len(set(y))\n",
    "        \n",
    "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
    "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
    "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
    "        \n",
    "        options = {'maxiter': self.maxiter}\n",
    "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
    "                    args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
    "        \n",
    "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        *unused_out, h = self._forward(X, self.t1, self.t2)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize a MLP classifier, specifying the conjugate gradient minimization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLP(method='CG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can confirm that it solves a non-linear classification, using the simple XOR example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = _xor[['x1','x2']].values\n",
    "y = _xor['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a somewhat more sophisiticated example, we can use the iris dataset included with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as datasets\n",
    "from sklearn import model_selection\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MLP()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used the scikit-learn interface, its easy to take advantage of the `metrics` module to evaluate the MLP's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network specification\n",
    "\n",
    "The MLP implemented above uses a single hidden layer, though it allows a user-specified number of hidden layer nodes (defaults to 25). It is worth considering whether it is useful having **multiple hidden layers**, and whether more hidden nodes is desirable.\n",
    "\n",
    "Unfortunately, there is no theory to guide the choice of hidden node number. As a result, we are left to experiment with this parameter, perhaps in some systematic fashion such as cross-validation.\n",
    "\n",
    "Adding additional layers presents only additional \"bookkeeping\" overhead to the user, with the weight updating becoming more complicated as layers are added. So, we don't want to add more hidden layers if it does not pay off in performance. It turns out that two or three layers (including the output layer) can be shown to approximate almost any smooth function. Combining 3 sigmoid functions allows local responses to be approximated with arbitrary accuracy. This is sufficient for determining any decision boundary.\n",
    "\n",
    "### Neural network validation\n",
    "\n",
    "Just as with other supervised learning algorithms, neural networks can be under- or over-fit to a training dataset. The degree to which a network is trained to a particular dataset depends on how long we train it on that dataset. Every time we run the MLP learning algorithm over a dataset (an **epoch**), it reduces the prediction error for that dataset. Thus, the number of epochs should be tuned as a hyperparameter, stopping when the testing-training error gap begins to widen.\n",
    "\n",
    "Note that though we can also use cross-validation to tune the number of hidden layers in the network, there is no risk of overfitting by having too many layers.\n",
    "\n",
    "### Example: Epoch tuning for MLPs\n",
    "\n",
    "The dataset `pima-indians-diabetes.data.txt` in your data folder contains eight measurements taken from a group of Pima Native Americans in Arizona, along with an indicator of the onset of diabetes. Use the MLP class to fit a neural network classifier to this dataset, and use cross-validation to examine the optimal number of epochs to use in training.\n",
    "\n",
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pima = pd.read_csv('../data/pima-indians-diabetes.data.txt', header=None)\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Mulitilayer perceptron in Theano\n",
    "\n",
    "Recall the Theano package, introduced earlier in the course; it is designed to evaluate expressions efficiently, and one of its key features is that it automatically differentiates expressions. This saves us from having to code a gradient function by hand. In principle, we could also use Theano to parallelize a neural network using GPUs, but we will not explore that here.\n",
    "\n",
    "### Layer class\n",
    "\n",
    "Again using an object-oriented approach, we will code our multilayer perceptron as a series of `Layer` classes, then apply each successively to the input to produce the network output.  The `Layer` class stores a weight matrix and a bias vector and includes a function for computing output.  \n",
    "\n",
    "Note that if we weren't using Theano, we might expect the `output` method to take in a vector and return the layer's activation in response to this input.  However, with Theano, the `output` function is instead meant to be used to _create_ (using `theano.function`) a function which can take in a vector and return the layer's activation.  So, if you were to pass, say, a `np.ndarray` to the `Layer` class's `output` method, it would raise an error.  Instead, we'll construct a function for actually computing the `Layer`'s activation outside of the class itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self, W_init, b_init, activation):\n",
    "        '''\n",
    "        A layer of a neural network, computes s(Wx + b) where s is a nonlinearity and x is the input vector.\n",
    "\n",
    "        :parameters:\n",
    "            - W_init : np.ndarray, shape=(n_output, n_input)\n",
    "                Values to initialize the weight matrix to.\n",
    "            - b_init : np.ndarray, shape=(n_output,)\n",
    "                Values to initialize the bias vector\n",
    "            - activation : theano.tensor.elemwise.Elemwise\n",
    "                Activation function for layer output\n",
    "        '''\n",
    "        \n",
    "        # Retrieve the input and output dimensionality based on W's initialization\n",
    "        n_output, n_input = W_init.shape\n",
    "        # Make sure b is n_output in size\n",
    "        assert b_init.shape == (n_output,)\n",
    "        \n",
    "        # All parameters should be shared variables.\n",
    "        # They're used in this class to compute the layer output,\n",
    "        # but are updated elsewhere when optimizing the network parameters.\n",
    "        # Note that we are explicitly requiring that W_init has the theano.config.floatX dtype\n",
    "        self.W = theano.shared(value=W_init.astype(theano.config.floatX),\n",
    "                               # The name parameter is solely for printing purporses\n",
    "                               name='W',\n",
    "                               # Setting borrow=True allows Theano to use user memory for this object.\n",
    "                               # It can make code slightly faster by avoiding a deep copy on construction.\n",
    "                               # For more details, see\n",
    "                               # http://deeplearning.net/software/theano/tutorial/aliasing.html\n",
    "                               borrow=True)\n",
    "        \n",
    "        # We can force our bias vector b to be a column vector using numpy's reshape method.\n",
    "        # When b is a column vector, we can pass a matrix-shaped input to the layer\n",
    "        # and get a matrix-shaped output, thanks to broadcasting (described below)\n",
    "        self.b = theano.shared(value=b_init.reshape(-1, 1).astype(theano.config.floatX),\n",
    "                               name='b',\n",
    "                               borrow=True,\n",
    "                               # Theano allows for broadcasting, similar to numpy.\n",
    "                               # However, you need to explicitly denote which axes can be broadcasted.\n",
    "                               # By setting broadcastable=(False, True), we are denoting that b\n",
    "                               # can be broadcast (copied) along its second dimension in order to be\n",
    "                               # added to another variable.  For more information, see\n",
    "                               # http://deeplearning.net/software/theano/library/tensor/basic.html\n",
    "                               broadcastable=(False, True))\n",
    "        \n",
    "        self.activation = activation\n",
    "        # We'll compute the gradient of the cost of the network with respect to the parameters in this list.\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def output(self, x):\n",
    "        '''\n",
    "        Compute this layer's output given an input\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for layer input\n",
    "\n",
    "        :returns:\n",
    "            - output : theano.tensor.var.TensorVariable\n",
    "                Mixed, biased, and activated x\n",
    "        '''\n",
    "        # Compute linear mix\n",
    "        lin_output = T.dot(self.W, x) + self.b\n",
    "        \n",
    "        # Output is just linear mix if no activation function\n",
    "        # Otherwise, apply the activation function\n",
    "        return (lin_output if self.activation is None else self.activation(lin_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP class\n",
    "\n",
    "Most of the functionality of our MLP is contained in the `Layer` class; the `MLP` class is essentially just a container for a list of `Layer`s and their parameters.  The `output` function simply recursively computes the output for each layer.  Finally, the `squared_error` returns the squared Euclidean distance between the output of the network given an input and the desired (ground truth) output.  This function is meant to be used as a cost in the setting of minimizing cost over some training data.  As above, the `output` and `squared error` functions are not to be used for actually computing values; instead, they're to be used to create functions which are used to compute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9, max_iteration=20):\n",
    "        '''\n",
    "        Multi-layer perceptron class, computes the composition of a sequence of Layers\n",
    "\n",
    "        :parameters:\n",
    "            - W_init : list of np.ndarray, len=N\n",
    "                Values to initialize the weight matrix in each layer to.\n",
    "                The layer sizes will be inferred from the shape of each matrix in W_init\n",
    "            - b_init : list of np.ndarray, len=N\n",
    "                Values to initialize the bias vector in each layer to\n",
    "            - activations : list of theano.tensor.elemwise.Elemwise, len=N\n",
    "                Activation function for layer output for each layer\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.max_iteration = max_iteration\n",
    "        \n",
    "    def output(self, x):\n",
    "        '''\n",
    "        Compute the MLP's output given an input\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for network input\n",
    "\n",
    "        :returns:\n",
    "            - output : theano.tensor.var.TensorVariable\n",
    "                x passed through the MLP\n",
    "        '''\n",
    "        # Recursively compute output\n",
    "        for layer in self.layers:\n",
    "            x = layer.output(x)\n",
    "        return x\n",
    "\n",
    "    def squared_error(self, x, y):\n",
    "        '''\n",
    "        Compute the squared euclidean error of the network output against the \"true\" output y\n",
    "        \n",
    "        :parameters:\n",
    "            - x : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for network input\n",
    "            - y : theano.tensor.var.TensorVariable\n",
    "                Theano symbolic variable for desired network output\n",
    "\n",
    "        :returns:\n",
    "            - error : theano.tensor.var.TensorVariable\n",
    "                The squared Euclidian distance between the network output and y\n",
    "        '''\n",
    "        return T.sum((self.output(x) - y)**2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # First, set the size of each layer (and the number of layers)\n",
    "        # Input layer size is training data dimensionality (2)\n",
    "        # Output size is just 1-d: class label - 0 or 1\n",
    "        # Finally, let the hidden layers be twice the size of the input.\n",
    "        # If we wanted more layers, we could just add another layer size to this list.\n",
    "        layer_sizes = [X.shape[0], X.shape[0]*2, 1]\n",
    "        # Set initial parameter values\n",
    "        W_init = []\n",
    "        b_init = []\n",
    "        activations = []\n",
    "        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            # Getting the correct initialization matters a lot for non-toy problems.\n",
    "            # However, here we can just use the following initialization with success:\n",
    "            # Normally distribute initial weights\n",
    "            W_init.append(np.random.randn(n_output, n_input))\n",
    "            # Set initial biases to 1\n",
    "            b_init.append(np.ones(n_output))\n",
    "            # We'll use sigmoid activation for all layers\n",
    "            # Note that this doesn't make a ton of sense when using squared distance\n",
    "            # because the sigmoid function is bounded on [0, 1].\n",
    "            activations.append(T.nnet.sigmoid)\n",
    "        \n",
    "        # Initialize lists of layers\n",
    "        self.layers = []\n",
    "        # Construct the layers\n",
    "        for W, b, activation in zip(W_init, b_init, activations):\n",
    "            self.layers.append(Layer(W, b, activation))\n",
    "\n",
    "        # Combine parameters from all layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "\n",
    "        # Create Theano variables for the MLP input\n",
    "        mlp_input = T.matrix('mlp_input')\n",
    "        # ... and the desired output\n",
    "        mlp_target = T.vector('mlp_target')\n",
    "\n",
    "        # Create a function for computing the cost of the network given an input\n",
    "        cost = self.squared_error(mlp_input, mlp_target)\n",
    "        # Create a theano function for training the network\n",
    "        train = theano.function([mlp_input, mlp_target], cost,\n",
    "                    updates=gradient_updates_momentum(cost, self.params, self.learning_rate, self.momentum))\n",
    "        # Create a theano function for computing the MLP's output given some input\n",
    "        mlp_output = theano.function([mlp_input], self.output(mlp_input))\n",
    "        \n",
    "        iteration = 0\n",
    "        # We'll only train the network with 20 iterations.\n",
    "        # A more common technique is to use a hold-out validation set.\n",
    "        # When the validation error starts to increase, the network is overfitting,\n",
    "        # so we stop training the net.  This is called \"early stopping\", which we won't do here.\n",
    "        while iteration < self.max_iteration:\n",
    "            # Train the network using the entire training set.\n",
    "            # With large datasets, it's much more common to use stochastic or mini-batch gradient descent\n",
    "            # where only a subset (or a single point) of the training set is used at each iteration.\n",
    "            # This can also help the network to avoid local minima.\n",
    "            self.current_cost_ = train(X, y)\n",
    "            \n",
    "            # Get the current network output for all points in the training set\n",
    "            self.current_output_ = mlp_output(X)\n",
    "            # We can compute the accuracy by thresholding the output\n",
    "            # and computing the proportion of points whose class match the ground truth class.\n",
    "            self.accuracy_ = np.mean((self.current_output_ > .5) == y)\n",
    "\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "To train the network, we will minimize the cost (squared Euclidean distance of network output vs. ground-truth) over a training set using gradient descent.  \n",
    "\n",
    "In Theano, we store the previous parameter update as a shared variable so that its value is preserved across iterations.  Then, during the gradient update, we not only update the parameters, but we also update the previous parameter update shared variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_updates_momentum(cost, params, learning_rate, momentum):\n",
    "    '''\n",
    "    Compute updates for gradient descent with momentum\n",
    "    \n",
    "    :parameters:\n",
    "        - cost : theano.tensor.var.TensorVariable\n",
    "            Theano cost function to minimize\n",
    "        - params : list of theano.tensor.var.TensorVariable\n",
    "            Parameters to compute gradient against\n",
    "        - learning_rate : float\n",
    "            Gradient descent learning rate\n",
    "        - momentum : float\n",
    "            Momentum parameter, should be at least 0 (standard gradient descent) and less than 1\n",
    "   \n",
    "    :returns:\n",
    "        updates : list\n",
    "            List of updates, one for each parameter\n",
    "    '''\n",
    "    # Make sure momentum is a sane value\n",
    "    assert momentum < 1 and momentum >= 0\n",
    "    # List of update steps for each parameter\n",
    "    updates = []\n",
    "    # Just gradient descent on cost\n",
    "    for param in params:\n",
    "        # For each parameter, we'll create a param_update shared variable.\n",
    "        # This variable will keep track of the parameter's update step across iterations.\n",
    "        # We initialize it to 0\n",
    "        param_update = theano.shared(param.get_value()*0., broadcastable=param.broadcastable)\n",
    "        # Each parameter is updated by taking a step in the direction of the gradient.\n",
    "        # However, we also \"mix in\" the previous step according to the given momentum value.\n",
    "        # Note that when updating param_update, we are using its old value and also the new gradient step.\n",
    "        updates.append((param, param - learning_rate*param_update))\n",
    "        # Note that we don't need to derive backpropagation to compute updates - just use T.grad!\n",
    "        updates.append((param_update, momentum*param_update + (1. - momentum)*T.grad(cost, param)))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example\n",
    "\n",
    "We'll train our neural network to classify two Gaussian-distributed clusters in 2d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training data - two randomly-generated Gaussian-distributed clouds of points in 2d space\n",
    "np.random.seed(0)\n",
    "# Number of points\n",
    "N = 1000\n",
    "# Labels for each cluster\n",
    "y = np.random.random_integers(0, 1, N)\n",
    "# Mean of each cluster\n",
    "means = np.array([[-1, 1], [-1, 1]])\n",
    "# Covariance (in X and Y direction) of each cluster\n",
    "covariances = np.random.random_sample((2, 2)) + 1\n",
    "# Dimensions of each point\n",
    "X = np.vstack([np.random.randn(N)*covariances[0, y] + means[0, y],\n",
    "               np.random.randn(N)*covariances[1, y] + means[1, y]])\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(*X, c=y, lw=.3, s=15, cmap=plt.cm.autumn)\n",
    "plt.axis([-6, 6, -6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Cost: {:.3f}, Accuracy: {:.3f}'.format(float(mlp.current_cost_), mlp.accuracy_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "T. Hastie, R. Tibshirani and J. Friedman. (2009) [Elements of Statistical Learning: Data Mining, Inference, and Prediction](http://statweb.stanford.edu/~tibs/ElemStatLearn/), second edition. Springer.\n",
    "\n",
    "S. Marsland. (2009) [Machine Learning: An Algorithmic Perspective](Machine Learning: An Algorithmic Perspectivehttp://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html). CRC Press.\n",
    "\n",
    "D. Rodriguez. (2013) [Basic [1 hidden layer] neural network on Python](http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/).\n",
    "\n",
    "D. Britz. (2015) [Implementing a Neural Network from Scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
